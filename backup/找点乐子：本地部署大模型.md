AI 发展到现在，已然一日千里。在本地部署一个看起来还不错的大模型，能做到什么程度呢？

我有一个树莓派 4B 8G，我想看看是否能有大模型能在此落地，它的表现又将如何呢？

综合对比，希望可用且流畅，我给出了最低门槛方案：

- 模型：Qwen2.5-1.5B 或 LLaMA-3.2-1B（4bit 量化）
- 框架：Ollama（底层是 llama.cpp）
- 硬件：
  - CPU：4 核即可
  - 内存：8GB 可用，6GB 勉强
  - GPU：不需要

尽管 Qwen2.5-1.5B 或 LLaMA-3.2-1B 都还不错，但考虑母语习惯，模型选择对中文支持更好的 Qwen2.5-1.5B 。

先在我的笔记本上跑跑看看效果。

Ollama 的 1.2 GB windows 安装包也太大了，考虑自己编译二进制文件。

```bash
git clone https://github.com/ggerganov/llama.cpp
```

源码约 235MB，如果只拉取必要代码还能更小。现在需要编译出较小体积的 llama-cli.exe
手动关闭掉一些不必要的选项：

```bash
cd llama.cpp
cmake -B build ^
  -DLLAMA_CURL=OFF ^
  -DLLAMA_CUDA=OFF ^
  -DLLAMA_OPENBLAS=OFF ^
  -DLLAMA_METAL=OFF
cmake --build build --config Release
```

最后得到一个约 3MB 大小的 `build/bin/Release/llama-cli.exe`

接下来就是导入模型了，到 HuggingFace，搜索：qwen2.5-1.5b-q4_k_m.gguf
这表示：

- 模型：Qwen2.5-1.5B
- 格式：GGUF（llama.cpp 专用）
- 量化：Q4_K_M（首选，体积/效果最优）

大小约 900 MB。

将下载好的文件 qwen2.5-1.5b-instruct-q4_k_m.gguf（文件名类似，可能略有差异）放入 llama.cpp/models 下

```bash
./build/bin/Release/llama-cli.exe -m ./models/qwen2.5-1.5b-instruct-q4_k_m.gguf -c 4096 -n 256 -t 6 
```

参数说明：
- -m：模型路径
- -t：线程数（= 物理核数）
- -c：上下文长度（4k 足够）
- -n：最大生成 token

<img width="410" height="357" alt="Image" src="https://github.com/user-attachments/assets/b51f5c7d-324b-4e11-8f84-349edad01216" />

enjoy it ! : )

